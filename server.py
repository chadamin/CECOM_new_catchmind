from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import torch
import clip
from PIL import Image
import io

app = FastAPI()

# âœ… CORS (ì›¹ ì—°ë™ í•„ìˆ˜)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],   # ê°œë°œ ë‹¨ê³„
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def root():
    return {"status": "server alive"}

# ğŸ”¹ CLIP ëª¨ë¸ì€ ì„œë²„ ì‹œì‘ ì‹œ 1ë²ˆë§Œ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# ğŸ”¹ ê²Œì„ì—ì„œ ì‚¬ìš©í•  í›„ë³´ ë‹¨ì–´ë“¤
CANDIDATES = [
    "cat",
    "dog",
    "car",
    "house",
    "tree",
    "person",
    "handwritten drawing"
]

@app.post("/clip-test")
async def clip_test(image: UploadFile = File(...)):
    # ì´ë¯¸ì§€ ë¡œë“œ
    image_bytes = await image.read()
    image_pil = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    image_input = preprocess(image_pil).unsqueeze(0).to(device)

    # CLIP í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    texts = [f"a drawing of a {c}" for c in CANDIDATES]
    text_tokens = clip.tokenize(texts).to(device)

    # CLIP ì¶”ë¡ 
    with torch.no_grad():
        image_features = model.encode_image(image_input)
        text_features = model.encode_text(text_tokens)
        similarity = (image_features @ text_features.T).softmax(dim=-1)[0]

    best_idx = similarity.argmax().item()

    # âœ… í”„ë¡ íŠ¸ì—ì„œ ë°”ë¡œ ì“°ê¸° ì¢‹ì€ í˜•íƒœ
    return {
        "guess": CANDIDATES[best_idx],
        "confidence": round(float(similarity[best_idx]), 2)
    }
